{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Ingesta y ProyecciÃ³n SocioeconÃ³mica (CASEN)\n",
    "\n",
    "**Objetivo:** Generar una serie temporal anual de ingresos y pobreza por comuna (2015-2025).\n",
    "\n",
    "**LÃ³gica de Procesamiento:**\n",
    "1. **2015 y 2017:** Se procesan directamente ya que contienen la informaciÃ³n de comuna integrada.\n",
    "2. **2022 (Caso Especial):** Se implementa un cruce de tres fuentes para reconstruir la ubicaciÃ³n:\n",
    "    * **Base Principal:** Ingresos (`ytot`) y Pobreza.\n",
    "    * **Base GeogrÃ¡fica:** Cruce por `folio` para obtener el cÃ³digo de `comuna`.\n",
    "    * **Libro de CÃ³digos (Excel):** TraducciÃ³n del cÃ³digo numÃ©rico al nombre real de la comuna.\n",
    "\n",
    "**Output:** `data/processed/socioeconomico_anual_completo.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadstat\n",
    "import os\n",
    "import unicodedata\n",
    "\n",
    "# --- CONFIGURACIÃ“N DE RUTAS ---\n",
    "RAW_DIR = os.path.join('..', 'data', 'raw')\n",
    "PROCESSED_DIR = os.path.join('..', 'data', 'processed')\n",
    "OUTPUT_FILE = os.path.join(PROCESSED_DIR, 'socioeconomico_anual_completo.csv')\n",
    "\n",
    "# Archivos 2015-2017\n",
    "FILE_2015 = os.path.join(RAW_DIR, 'Casen 2015.sav')\n",
    "FILE_2017 = os.path.join(RAW_DIR, 'Casen 2017.sav')\n",
    "\n",
    "# Archivos 2022 (El set completo)\n",
    "FILE_2022_MAIN = os.path.join(RAW_DIR, 'Base de datos Casen 2022 SPSS_18 marzo 2024.sav')\n",
    "FILE_2022_COMUNAS = os.path.join(RAW_DIR, 'Base de datos provincia y comuna Casen 2022 SPSS.sav')\n",
    "FILE_2022_CODIGOS = os.path.join(RAW_DIR, 'Libro de codigos Base de datos provincia y comuna Casen 2022.xlsx')\n",
    "\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Funciones Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_texto(texto):\n",
    "    \"\"\"Normaliza nombres para cruce (minÃºsculas, sin tildes).\"\"\"\n",
    "    if pd.isna(texto):\n",
    "        return \"\"\n",
    "    texto = str(texto).lower().strip()\n",
    "    texto = unicodedata.normalize('NFKD', texto).encode('ASCII', 'ignore').decode('utf-8')\n",
    "    return texto\n",
    "\n",
    "def calcular_agregados(df, anio, col_comuna='nombre_comuna'):\n",
    "    \"\"\"Agrupa por comuna y calcula promedios.\"\"\"\n",
    "    # Estandarizar nombres de columnas para el cÃ¡lculo\n",
    "    df.rename(columns={c: c.lower() for c in df.columns}, inplace=True)\n",
    "    col_comuna = col_comuna.lower()\n",
    "    \n",
    "    # Flag de Pobreza (1 y 2 suelen ser pobreza extrema y no extrema)\n",
    "    if 'pobreza' in df.columns:\n",
    "        df['es_pobre'] = df['pobreza'].isin([1, 2]).astype(int)\n",
    "    else:\n",
    "        df['es_pobre'] = 0 # Fallback si no existe\n",
    "\n",
    "    # Agrupar\n",
    "    resumen = df.groupby(col_comuna).agg(\n",
    "        ingreso_promedio=('ytot', 'mean'),\n",
    "        tasa_pobreza=('es_pobre', 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    resumen.rename(columns={col_comuna: 'nombre_comuna'}, inplace=True)\n",
    "    resumen['anio'] = anio\n",
    "    return resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Procesamiento 2022: LÃ³gica de Triple Archivo\n",
    "AquÃ­ aplicamos la lÃ³gica de rescatar los nombres reales usando el Excel de cÃ³digos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_diccionario_excel_2022():\n",
    "    \"\"\"\n",
    "    Lee el Excel 'Libro de codigos...' para mapear CÃ³digo -> Nombre.\n",
    "    Logica recuperada: Leer desde fila 7, columnas 3 y 4.\n",
    "    \"\"\"\n",
    "    print(f\"  ðŸ“– Leyendo diccionario: {os.path.basename(FILE_2022_CODIGOS)}\")\n",
    "    try:\n",
    "        # Leemos sin header para usar iloc puro como en tu ejemplo exitoso\n",
    "        df_codigos = pd.read_excel(FILE_2022_CODIGOS, header=None)\n",
    "        \n",
    "        # Extraemos datos desde la fila 7 (Ã­ndice 6) y columnas C y D (Ã­ndices 3 y 4)\n",
    "        df_clean = df_codigos.iloc[6:, [3, 4]].copy()\n",
    "        df_clean.columns = ['codigo', 'nombre']\n",
    "        \n",
    "        # Limpieza\n",
    "        df_clean.dropna(subset=['codigo'], inplace=True)\n",
    "        # Convertir cÃ³digo a entero para coincidir con el SAV\n",
    "        df_clean['codigo'] = pd.to_numeric(df_clean['codigo'], errors='coerce')\n",
    "        df_clean.dropna(subset=['codigo'], inplace=True)\n",
    "        df_clean['codigo'] = df_clean['codigo'].astype(int)\n",
    "        \n",
    "        # Crear diccionario {13101: 'Santiago'}\n",
    "        diccionario = pd.Series(df_clean['nombre'].values, index=df_clean['codigo']).to_dict()\n",
    "        print(f\"    -> Diccionario cargado con {len(diccionario)} comunas.\")\n",
    "        return diccionario\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    âŒ Error leyendo Excel de cÃ³digos: {e}\")\n",
    "        return {}\n",
    "\n",
    "def procesar_2022():\n",
    "    print(\"--- Procesando 2022 ---\")\n",
    "    \n",
    "    # 1. Cargar Datos Principales (Ingresos)\n",
    "    print(\"  Cargando Base Principal...\")\n",
    "    df_main, _ = pyreadstat.read_sav(FILE_2022_MAIN, usecols=['folio', 'ytot', 'pobreza'])\n",
    "    \n",
    "    # 2. Cargar Datos GeogrÃ¡ficos (Para obtener el cÃ³digo de comuna)\n",
    "    print(\"  Cargando Base Comunas (GeogrÃ¡fica)...\")\n",
    "    df_geo, _ = pyreadstat.read_sav(FILE_2022_COMUNAS, usecols=['folio', 'comuna', 'region'])\n",
    "    \n",
    "    # 3. Unir por Folio\n",
    "    print(\"  Fusionando datasets por Folio...\")\n",
    "    df_full = pd.merge(df_main, df_geo, on='folio', how='inner')\n",
    "    \n",
    "    # 4. Filtrar RM\n",
    "    df_rm = df_full[df_full['region'] == 13].copy()\n",
    "    print(f\"  Registros en RM: {len(df_rm)}\")\n",
    "    \n",
    "    # 5. Aplicar TraducciÃ³n de Nombres (Excel)\n",
    "    mapa_nombres = cargar_diccionario_excel_2022()\n",
    "    \n",
    "    if mapa_nombres:\n",
    "        # Asegurar que el cÃ³digo en el DF sea int para el mapeo\n",
    "        df_rm['comuna_int'] = pd.to_numeric(df_rm['comuna'], errors='coerce').fillna(0).astype(int)\n",
    "        df_rm['nombre_real'] = df_rm['comuna_int'].map(mapa_nombres)\n",
    "        \n",
    "        # Rellenar no encontrados con el cÃ³digo original\n",
    "        df_rm['nombre_real'] = df_rm['nombre_real'].fillna(df_rm['comuna'].astype(str))\n",
    "    else:\n",
    "        df_rm['nombre_real'] = df_rm['comuna'].astype(str)\n",
    "\n",
    "    return calcular_agregados(df_rm, 2022, col_comuna='nombre_real')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Procesamiento 2015 y 2017\n",
    "LÃ³gica estÃ¡ndar para los archivos antiguos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_historico(anio, ruta):\n",
    "    print(f\"--- Procesando {anio} ---\")\n",
    "    try:\n",
    "        # Leer columnas clave (a veces 'comuna' trae el nombre en labels)\n",
    "        df, meta = pyreadstat.read_sav(ruta, disable_datetime_conversion=True)\n",
    "        df.rename(columns={c: c.lower() for c in df.columns}, inplace=True)\n",
    "        \n",
    "        # Filtrar RM\n",
    "        if 'region' in df.columns:\n",
    "            df = df[df['region'] == 13].copy()\n",
    "            \n",
    "        # Intentar obtener nombres de los metadatos del SAV\n",
    "        col_nombre = 'comuna' # Default\n",
    "        if 'comuna' in meta.variable_value_labels:\n",
    "            df['nombre_comuna'] = df['comuna'].map(meta.variable_value_labels['comuna'])\n",
    "            col_nombre = 'nombre_comuna'\n",
    "        \n",
    "        return calcular_agregados(df, anio, col_comuna=col_nombre)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error en {anio}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. EjecuciÃ³n, InterpolaciÃ³n y Guardado\n",
    "Juntamos todo y rellenamos los aÃ±os faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Procesando 2015 ---\n",
      "--- Procesando 2017 ---\n",
      "--- Procesando 2022 ---\n",
      "  Cargando Base Principal...\n",
      "  Cargando Base Comunas (GeogrÃ¡fica)...\n",
      "  Fusionando datasets por Folio...\n",
      "âŒ FallÃ³ proceso 2022: 'region'\n",
      "\n",
      ">>> Interpolando serie temporal (2015-2025)...\n",
      "Guardando archivo final: ..\\data\\processed\\socioeconomico_anual_completo.csv\n",
      "âœ… Proceso finalizado. Muestra:\n",
      "   comuna_norm  anio nombre_comuna  ingreso_promedio  tasa_pobreza\n",
      "0        alhue  2015         AlhuÃ©     281611.951900      0.087838\n",
      "1        alhue  2016         AlhuÃ©     328840.744311      0.063450\n",
      "2        alhue  2017         AlhuÃ©     376069.536723      0.039062\n",
      "3        alhue  2018         AlhuÃ©     376069.536723      0.039062\n",
      "4        alhue  2019         AlhuÃ©     376069.536723      0.039062\n",
      "5        alhue  2020         AlhuÃ©     376069.536723      0.039062\n",
      "6        alhue  2021         AlhuÃ©     376069.536723      0.039062\n",
      "7        alhue  2022         AlhuÃ©     376069.536723      0.039062\n",
      "8        alhue  2023         AlhuÃ©     376069.536723      0.039062\n",
      "9        alhue  2024         AlhuÃ©     376069.536723      0.039062\n",
      "10       alhue  2025         AlhuÃ©     376069.536723      0.039062\n",
      "11        buin  2015          Buin     370675.200000      0.122807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\corre\\AppData\\Local\\Temp\\ipykernel_8052\\2291325581.py:46: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_interpolado = df_final.groupby('comuna_norm').apply(rellenar_grupo).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "lista_dfs = []\n",
    "\n",
    "# 1. Ejecutar procesos\n",
    "df_15 = procesar_historico(2015, FILE_2015)\n",
    "if df_15 is not None: lista_dfs.append(df_15)\n",
    "\n",
    "df_17 = procesar_historico(2017, FILE_2017)\n",
    "if df_17 is not None: lista_dfs.append(df_17)\n",
    "\n",
    "# 2022 (Complejo)\n",
    "try:\n",
    "    df_22 = procesar_2022()\n",
    "    if df_22 is not None: lista_dfs.append(df_22)\n",
    "except Exception as e:\n",
    "    print(f\"âŒ FallÃ³ proceso 2022: {e}\")\n",
    "\n",
    "# 2. Consolidar\n",
    "if not lista_dfs:\n",
    "    raise ValueError(\"No hay datos para procesar.\")\n",
    "\n",
    "df_base = pd.concat(lista_dfs, ignore_index=True)\n",
    "\n",
    "# 3. Normalizar para asegurar que 'Santiago' 2015 se una con 'santiago' 2022\n",
    "df_base['comuna_norm'] = df_base['nombre_comuna'].apply(normalizar_texto)\n",
    "\n",
    "# 4. InterpolaciÃ³n (Rellenar aÃ±os faltantes)\n",
    "print(\"\\n>>> Interpolando serie temporal (2015-2025)...\")\n",
    "\n",
    "# Crear esqueleto completo\n",
    "comunas = df_base['comuna_norm'].unique()\n",
    "anios = range(2015, 2026)\n",
    "grid = pd.MultiIndex.from_product([comunas, anios], names=['comuna_norm', 'anio']).to_frame(index=False)\n",
    "\n",
    "# Unir y rellenar\n",
    "df_final = pd.merge(grid, df_base, on=['comuna_norm', 'anio'], how='left')\n",
    "\n",
    "def rellenar_grupo(g):\n",
    "    g = g.sort_values('anio')\n",
    "    # Rellenar huecos internos (2016, 2018-2021) linealmente\n",
    "    g['ingreso_promedio'] = g['ingreso_promedio'].interpolate(method='linear')\n",
    "    g['tasa_pobreza'] = g['tasa_pobreza'].interpolate(method='linear')\n",
    "    # Proyectar hacia el futuro (2023-2025) repitiendo el Ãºltimo dato\n",
    "    g = g.ffill().bfill()\n",
    "    return g\n",
    "\n",
    "df_interpolado = df_final.groupby('comuna_norm').apply(rellenar_grupo).reset_index(drop=True)\n",
    "\n",
    "# Guardar\n",
    "print(f\"Guardando archivo final: {OUTPUT_FILE}\")\n",
    "df_interpolado.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"âœ… Proceso finalizado. Muestra:\")\n",
    "print(df_interpolado.head(12))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Ingesta y ProyecciÃ³n SocioeconÃ³mica (CASEN)\n",
    "\n",
    "**Objetivo:** Generar la serie temporal completa (2015-2025) de ingresos y pobreza por comuna.\n",
    "\n",
    "**CorrecciÃ³n 2022:** La variable `region` se extrae desde la base de datos principal, y se cruza con la base geogrÃ¡fica solo para obtener la `comuna`.\n",
    "\n",
    "**Fuentes:**\n",
    "1. **2015/2017:** Metadatos internos (.sav).\n",
    "2. **2020:** Base .sav + Diccionario Excel (Hoja 'HdR').\n",
    "3. **2022:** FusiÃ³n Base Principal (con RegiÃ³n) + Base Geo (Comuna) + Diccionario Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadstat\n",
    "import os\n",
    "import unicodedata\n",
    "\n",
    "# --- CONFIGURACIÃ“N ---\n",
    "RAW_DIR = os.path.join('..', 'data', 'raw')\n",
    "PROCESSED_DIR = os.path.join('..', 'data', 'processed')\n",
    "OUTPUT_FILE = os.path.join(PROCESSED_DIR, 'socioeconomico_anual_completo.csv')\n",
    "\n",
    "# Archivos\n",
    "FILES = {\n",
    "    '2015': os.path.join(RAW_DIR, 'Casen 2015.sav'),\n",
    "    '2017': os.path.join(RAW_DIR, 'Casen 2017.sav'),\n",
    "    '2020_data': os.path.join(RAW_DIR, 'Base_de_datos_Casen_2020.sav'),\n",
    "    '2020_codigos': os.path.join(RAW_DIR, 'libro_de_codigos_base_de_datos_casen_2020.xlsx'),\n",
    "    '2022_main': os.path.join(RAW_DIR, 'Base de datos Casen 2022 SPSS_18 marzo 2024.sav'),\n",
    "    '2022_geo': os.path.join(RAW_DIR, 'Base de datos provincia y comuna Casen 2022 SPSS.sav'),\n",
    "    '2022_codigos': os.path.join(RAW_DIR, 'Libro de codigos Base de datos provincia y comuna Casen 2022.xlsx')\n",
    "}\n",
    "\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Funciones Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_texto(texto):\n",
    "    if pd.isna(texto): return \"\"\n",
    "    texto = str(texto).lower().strip()\n",
    "    texto = unicodedata.normalize('NFKD', texto).encode('ASCII', 'ignore').decode('utf-8')\n",
    "    return texto\n",
    "\n",
    "def calcular_metricas(df, anio, col_comuna='nombre_comuna'):\n",
    "    # Estandarizar y calcular mÃ©tricas\n",
    "    df.rename(columns={c: c.lower() for c in df.columns}, inplace=True)\n",
    "    col_comuna = col_comuna.lower()\n",
    "    \n",
    "    # Pobreza: 1=Extrema, 2=No Extrema -> Ambos son Pobreza\n",
    "    if 'pobreza' in df.columns:\n",
    "        df['es_pobre'] = df['pobreza'].isin([1, 2]).astype(int)\n",
    "    else:\n",
    "        df['es_pobre'] = 0\n",
    "\n",
    "    resumen = df.groupby(col_comuna).agg(\n",
    "        ingreso_promedio=('ytot', 'mean'),\n",
    "        tasa_pobreza=('es_pobre', 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    resumen.rename(columns={col_comuna: 'nombre_comuna'}, inplace=True)\n",
    "    resumen['anio'] = int(anio)\n",
    "    return resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extractores de Diccionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_diccionario_2020(ruta_excel):\n",
    "    \"\"\"Extrae cÃ³digos de la hoja 'HdR' buscando el ancla 1101.\"\"\"\n",
    "    print(f\"  ðŸ“– Leyendo cÃ³digos 2020 (HdR): {os.path.basename(ruta_excel)}\")\n",
    "    try:\n",
    "        df = pd.read_excel(ruta_excel, sheet_name='HdR', header=None)\n",
    "        mask = df.astype(str).apply(lambda x: x.str.contains('^1101$|^1101.0$', regex=True))\n",
    "        coords = np.argwhere(mask.values)\n",
    "        \n",
    "        if len(coords) > 0:\n",
    "            row_start, col_idx = coords[0]\n",
    "            data_block = df.iloc[row_start:, [col_idx, col_idx+1]].copy()\n",
    "            data_block.columns = ['codigo', 'nombre']\n",
    "            data_block = data_block.dropna()\n",
    "            data_block['codigo'] = pd.to_numeric(data_block['codigo'], errors='coerce')\n",
    "            data_block = data_block.dropna(subset=['codigo'])\n",
    "            diccionario = pd.Series(data_block.nombre.values, index=data_block.codigo.astype(int)).to_dict()\n",
    "            return diccionario\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"    âŒ Error 2020 Excel: {e}\"); return {}\n",
    "\n",
    "def extraer_diccionario_2022(ruta_excel):\n",
    "    \"\"\"Extrae cÃ³digos 2022. Se busca tambiÃ©n el ancla 1101 por seguridad.\"\"\"\n",
    "    print(f\"  ðŸ“– Leyendo cÃ³digos 2022: {os.path.basename(ruta_excel)}\")\n",
    "    try:\n",
    "        # Intentar lectura directa primero (fila 7 aprox)\n",
    "        df = pd.read_excel(ruta_excel, header=None)\n",
    "        \n",
    "        # BÃºsqueda de ancla 1101 (Iquique) para ser infalible\n",
    "        mask = df.astype(str).apply(lambda x: x.str.contains('^1101$|^1101.0$', regex=True))\n",
    "        coords = np.argwhere(mask.values)\n",
    "        \n",
    "        if len(coords) > 0:\n",
    "            row_start, col_idx = coords[0]\n",
    "            # Asumimos col_idx = codigo, col_idx+1 = nombre\n",
    "            data = df.iloc[row_start:, [col_idx, col_idx+1]].copy()\n",
    "            data.columns = ['codigo', 'nombre']\n",
    "            data = data.dropna()\n",
    "            data['codigo'] = pd.to_numeric(data['codigo'], errors='coerce')\n",
    "            data = data.dropna(subset=['codigo'])\n",
    "            return pd.Series(data.nombre.values, index=data.codigo.astype(int)).to_dict()\n",
    "        \n",
    "        # Fallback a lectura fija si no encuentra ancla\n",
    "        print(\"    âš ï¸ Ancla no encontrada, usando iloc fijo [6:, 3:5]\")\n",
    "        df_clean = df.iloc[6:, [3, 4]].copy()\n",
    "        df_clean.columns = ['codigo', 'nombre']\n",
    "        df_clean = df_clean.dropna()\n",
    "        df_clean['codigo'] = pd.to_numeric(df_clean['codigo'], errors='coerce')\n",
    "        df_clean.dropna(inplace=True)\n",
    "        return pd.Series(df_clean.nombre.values, index=df_clean.codigo.astype(int)).to_dict()\n",
    "        \n",
    "    except Exception as e: \n",
    "        print(f\"    âŒ Error 2022 Excel: {e}\"); return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Procesamiento por AÃ±o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_historico(anio, ruta):\n",
    "    # 2015 y 2017\n",
    "    print(f\"--- Procesando {anio} ---\")\n",
    "    try:\n",
    "        df, meta = pyreadstat.read_sav(ruta, disable_datetime_conversion=True)\n",
    "        df.columns = [c.lower() for c in df.columns]\n",
    "        if 'region' in df: df = df[df['region'] == 13].copy()\n",
    "        \n",
    "        col_nombre = 'comuna'\n",
    "        if 'comuna' in meta.variable_value_labels:\n",
    "            df['nombre_comuna'] = df['comuna'].map(meta.variable_value_labels['comuna'])\n",
    "            col_nombre = 'nombre_comuna'\n",
    "            \n",
    "        return calcular_metricas(df, anio, col_comuna=col_nombre)\n",
    "    except Exception as e: \n",
    "        print(f\"Error {anio}: {e}\"); return None\n",
    "\n",
    "def procesar_2020():\n",
    "    # 2020 (HÃ­brido)\n",
    "    print(\"--- Procesando 2020 ---\")\n",
    "    try:\n",
    "        df, _ = pyreadstat.read_sav(FILES['2020_data'], usecols=['region', 'comuna', 'ytot', 'pobreza'])\n",
    "        df.columns = [c.lower() for c in df.columns]\n",
    "        df = df[df['region'] == 13].copy()\n",
    "        \n",
    "        mapa = extraer_diccionario_2020(FILES['2020_codigos'])\n",
    "        if mapa:\n",
    "            df['nombre_comuna'] = df['comuna'].map(mapa).fillna(df['comuna'].astype(str))\n",
    "        else:\n",
    "            df['nombre_comuna'] = df['comuna'].astype(str)\n",
    "            \n",
    "        return calcular_metricas(df, 2020, col_comuna='nombre_comuna')\n",
    "    except Exception as e:\n",
    "        print(f\"Error 2020: {e}\"); return None\n",
    "\n",
    "def procesar_2022():\n",
    "    # 2022 (Triple Merge)\n",
    "    print(\"--- Procesando 2022 ---\")\n",
    "    try:\n",
    "        # 1. Cargar Main (Incluye RegiÃ³n, Ingresos, Pobreza)\n",
    "        print(\"  Cargando Main...\")\n",
    "        df_main, _ = pyreadstat.read_sav(FILES['2022_main'], usecols=['folio', 'ytot', 'pobreza', 'region'])\n",
    "        \n",
    "        # 2. Cargar Geo (Incluye Comuna, Folio)\n",
    "        print(\"  Cargando Geo...\")\n",
    "        df_geo, _ = pyreadstat.read_sav(FILES['2022_geo'], usecols=['folio', 'comuna'])\n",
    "        \n",
    "        # 3. Merge\n",
    "        df = pd.merge(df_main, df_geo, on='folio')\n",
    "        \n",
    "        # 4. Filtrar RM (usando la regiÃ³n del main)\n",
    "        df = df[df['region'] == 13].copy()\n",
    "        \n",
    "        # 5. Mapear\n",
    "        mapa = extraer_diccionario_2022(FILES['2022_codigos'])\n",
    "        if mapa:\n",
    "            df['nombre_comuna'] = df['comuna'].map(mapa).fillna(df['comuna'].astype(str))\n",
    "        else:\n",
    "            df['nombre_comuna'] = df['comuna'].astype(str)\n",
    "        \n",
    "        return calcular_metricas(df, 2022)\n",
    "    except Exception as e:\n",
    "        print(f\"Error 2022: {e}\"); return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ConsolidaciÃ³n e InterpolaciÃ³n Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Procesando 2015 ---\n",
      "--- Procesando 2017 ---\n",
      "--- Procesando 2020 ---\n",
      "  ðŸ“– Leyendo cÃ³digos 2020 (HdR): libro_de_codigos_base_de_datos_casen_2020.xlsx\n",
      "--- Procesando 2022 ---\n",
      "  Cargando Main...\n",
      "  Cargando Geo...\n",
      "  ðŸ“– Leyendo cÃ³digos 2022: Libro de codigos Base de datos provincia y comuna Casen 2022.xlsx\n",
      "\n",
      ">>> Generando serie temporal completa...\n",
      "Guardando: ..\\data\\processed\\socioeconomico_anual_completo.csv\n",
      "âœ… Proceso finalizado.\n",
      "       comuna_norm  anio nombre_comuna  ingreso_promedio  tasa_pobreza\n",
      "562       vitacura  2016           NaN      1.831720e+06      0.000000\n",
      "412    puente alto  2020   Puente Alto      2.423730e+05      0.096547\n",
      "385    providencia  2015   Providencia      1.412648e+06      0.008512\n",
      "53     cerro navia  2024           NaN      3.012317e+05      0.034448\n",
      "166     la florida  2016           NaN      4.409886e+05      0.037260\n",
      "232   lo barnechea  2016           NaN      9.631456e+05      0.035612\n",
      "457          renca  2021           NaN      2.084882e+05      0.067904\n",
      "147  isla de maipo  2019           NaN      2.040749e+05      0.070647\n",
      "9            alhue  2024           NaN      3.341134e+05      0.152448\n",
      "464   san bernardo  2017  San Bernardo      3.134447e+05      0.094965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\corre\\AppData\\Local\\Temp\\ipykernel_2348\\3780308756.py:27: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_final = df_merged.groupby('comuna_norm').apply(rellenar_grupo).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "dfs.append(procesar_historico(2015, FILES['2015']))\n",
    "dfs.append(procesar_historico(2017, FILES['2017']))\n",
    "dfs.append(procesar_2020())\n",
    "dfs.append(procesar_2022())\n",
    "\n",
    "# Unir resultados\n",
    "df_base = pd.concat([d for d in dfs if d is not None], ignore_index=True)\n",
    "df_base['comuna_norm'] = df_base['nombre_comuna'].apply(normalizar_texto)\n",
    "\n",
    "# --- InterpolaciÃ³n ---\n",
    "print(\"\\n>>> Generando serie temporal completa...\")\n",
    "comunas = df_base['comuna_norm'].unique()\n",
    "anios = range(2015, 2026)\n",
    "grid = pd.MultiIndex.from_product([comunas, anios], names=['comuna_norm', 'anio']).to_frame(index=False)\n",
    "\n",
    "# Merge del Grid con los Datos Reales\n",
    "df_merged = pd.merge(grid, df_base, on=['comuna_norm', 'anio'], how='left')\n",
    "\n",
    "def rellenar_grupo(g):\n",
    "    g = g.sort_values('anio')\n",
    "    g['ingreso_promedio'] = g['ingreso_promedio'].interpolate(method='linear').ffill().bfill()\n",
    "    g['tasa_pobreza'] = g['tasa_pobreza'].interpolate(method='linear').ffill().bfill()\n",
    "    return g\n",
    "\n",
    "# Aplicar interpolaciÃ³n usando la variable correcta\n",
    "df_final = df_merged.groupby('comuna_norm').apply(rellenar_grupo).reset_index(drop=True)\n",
    "\n",
    "print(f\"Guardando: {OUTPUT_FILE}\")\n",
    "df_final.to_csv(OUTPUT_FILE, index=False)\n",
    "print(\"âœ… Proceso finalizado.\")\n",
    "print(df_final.sample(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
